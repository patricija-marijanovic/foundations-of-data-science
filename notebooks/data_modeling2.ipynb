{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# DATA MODELING - 2",
   "id": "f47eb3bdaa6539cb"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-19T10:09:34.624610Z",
     "start_time": "2025-11-19T10:09:34.613422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T10:16:08.098355Z",
     "start_time": "2025-11-19T10:16:08.086206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Paths\n",
    "project_root = Path(os.getcwd()).parents[0]\n",
    "data_folder = project_root / 'data' / 'data_accidents'\n",
    "grid_weather_dir = data_folder / \"grid_weather\"\n",
    "os.makedirs(grid_weather_dir, exist_ok=True)"
   ],
   "id": "b5a209003b1df14d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T10:16:57.141194Z",
     "start_time": "2025-11-19T10:16:56.674606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# You'll need these later\n",
    "try:\n",
    "    import openmeteo_requests\n",
    "    import requests_cache\n",
    "    from retry_requests import retry\n",
    "except ImportError:\n",
    "    !pip install openmeteo-requests requests-cache retry-requests\n",
    "\n",
    "# Setup Open-Meteo client (from your original code)\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after=86400)\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)"
   ],
   "id": "13cbe91c309e9b07",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "7d4ed400d6fc1041"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T10:21:51.258127Z",
     "start_time": "2025-11-19T10:21:51.235183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### STEP 1: Check FARS columns across years (corrected for LONGITUD)\n",
    "years_to_check = [\"1980\", \"1990\", \"2000\", \"2010\", \"2020\"]\n",
    "for year in years_to_check:\n",
    "    folder = data_folder / year\n",
    "    acc_file = folder / 'accident.csv'\n",
    "    if acc_file.exists():\n",
    "        try:\n",
    "            df = pd.read_csv(acc_file, nrows=1, encoding='utf-8')\n",
    "        except:\n",
    "            df = pd.read_csv(acc_file, nrows=1, encoding='latin1')\n",
    "        print(f\"\\n{year} columns:\")\n",
    "        print(f\"  Has LAT/LON: {'LATITUDE' in df.columns and 'LONGITUD' in df.columns}\")\n",
    "        print(f\"  Has YEAR/MONTH/DAY: {all(c in df.columns for c in ['YEAR', 'MONTH', 'DAY'])}\")\n",
    "        print(f\"  Has DATE: {'DATE' in df.columns}\")\n",
    "        if 'WEATHER' in df.columns:\n",
    "            print(f\"  Has WEATHER condition: True\")"
   ],
   "id": "bb67770d43c1009f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1980 columns:\n",
      "  Has LAT/LON: False\n",
      "  Has YEAR/MONTH/DAY: True\n",
      "  Has DATE: False\n",
      "  Has WEATHER condition: True\n",
      "\n",
      "1990 columns:\n",
      "  Has LAT/LON: False\n",
      "  Has YEAR/MONTH/DAY: True\n",
      "  Has DATE: False\n",
      "  Has WEATHER condition: True\n",
      "\n",
      "2000 columns:\n",
      "  Has LAT/LON: True\n",
      "  Has YEAR/MONTH/DAY: True\n",
      "  Has DATE: False\n",
      "  Has WEATHER condition: True\n",
      "\n",
      "2010 columns:\n",
      "  Has LAT/LON: True\n",
      "  Has YEAR/MONTH/DAY: True\n",
      "  Has DATE: False\n",
      "  Has WEATHER condition: True\n",
      "\n",
      "2020 columns:\n",
      "  Has LAT/LON: True\n",
      "  Has YEAR/MONTH/DAY: True\n",
      "  Has DATE: False\n",
      "  Has WEATHER condition: True\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T10:25:38.932184Z",
     "start_time": "2025-11-19T10:25:38.921923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "STATE_CENTROIDS = {\n",
    "    1: (32.3182, -86.9023),   # Alabama\n",
    "    2: (64.2008, -149.4853),  # Alaska\n",
    "    4: (34.0489, -111.0937),  # Arizona\n",
    "    5: (34.7996, -92.1849),   # Arkansas\n",
    "    6: (37.2719, -119.2702),  # California\n",
    "    8: (39.5501, -105.7821),  # Colorado\n",
    "    9: (41.6032, -73.0877),   # Connecticut\n",
    "    10: (39.1398, -75.5052),  # Delaware\n",
    "    11: (38.8974, -77.0268),  # District of Columbia\n",
    "    12: (27.6648, -81.5158),  # Florida\n",
    "    13: (32.1656, -82.9001),  # Georgia\n",
    "    15: (19.8968, -155.5828), # Hawaii\n",
    "    16: (44.0682, -114.7420), # Idaho\n",
    "    17: (39.7837, -89.6501),  # Illinois\n",
    "    18: (39.7910, -86.1490),  # Indiana\n",
    "    19: (41.8780, -93.0977),  # Iowa\n",
    "    20: (38.5266, -96.7265),  # Kansas\n",
    "    21: (37.8393, -85.7170),  # Kentucky\n",
    "    22: (31.2448, -92.1450),  # Louisiana\n",
    "    23: (45.2538, -69.4455),  # Maine\n",
    "    24: (39.0458, -76.6413),  # Maryland\n",
    "    25: (42.4072, -71.3824),  # Massachusetts\n",
    "    26: (43.3266, -84.5361),  # Michigan\n",
    "    27: (45.6945, -93.9002),  # Minnesota\n",
    "    28: (32.3547, -90.0602),  # Mississippi\n",
    "    29: (38.5443, -92.2884),  # Missouri\n",
    "    30: (46.9653, -110.0845), # Montana\n",
    "    31: (41.4925, -99.9018),  # Nebraska\n",
    "    32: (38.8026, -116.4194), # Nevada\n",
    "    33: (43.1939, -71.5724),  # New Hampshire\n",
    "    34: (40.2237, -74.7647),  # New Jersey\n",
    "    35: (34.5199, -106.0186), # New Mexico\n",
    "    36: (42.7512, -75.7634),  # New York\n",
    "    37: (35.7718, -80.0211),  # North Carolina\n",
    "    38: (47.5515, -100.4659), # North Dakota\n",
    "    39: (40.4173, -82.9071),  # Ohio\n",
    "    40: (35.0078, -97.0929),  # Oklahoma\n",
    "    41: (44.0009, -120.5542), # Oregon\n",
    "    42: (40.5908, -77.2098),  # Pennsylvania\n",
    "    44: (41.5801, -71.4774),  # Rhode Island\n",
    "    45: (33.8361, -81.1637),  # South Carolina\n",
    "    46: (44.5000, -100.0000), # South Dakota\n",
    "    47: (35.5175, -86.5804),  # Tennessee\n",
    "    48: (31.9686, -99.9018),  # Texas\n",
    "    49: (40.1500, -111.8625), # Utah\n",
    "    50: (44.5588, -72.5778),  # Vermont\n",
    "    51: (37.4316, -78.6569),  # Virginia\n",
    "    53: (47.7511, -120.7401), # Washington\n",
    "    54: (38.5976, -80.4549),  # West Virginia\n",
    "    55: (44.5000, -89.5000),  # Wisconsin\n",
    "    56: (43.0000, -107.5000)  #¬†Wyoming\n",
    "}"
   ],
   "id": "e98c28cf0f043032",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This step constructs a unified **daily fatal crash dataset** that combines two spatial strategies based on FARS data availability:\n",
    "\n",
    "- **1975‚Äì1999**: FARS does **not** include precise crash coordinates.\n",
    "  ‚Üí We assign each crash to its **state centroid** (using predefined latitude/longitude from `STATE_CENTROIDS`).\n",
    "\n",
    "- **2000‚Äì2023**: FARS includes **actual crash coordinates** (`LATITUDE` and `LONGITUD`).\n",
    "  ‚Üí We use these exact locations (after filtering for valid U.S. coordinates: 24¬∞N‚Äì50¬∞N, 125¬∞W‚Äì66¬∞W).\n",
    "\n",
    "#### Key Processing Steps:\n",
    "1. **Parse date** from `YEAR`, `MONTH`, and `DAY` columns (excluding invalid dates like `DAY = 0`).\n",
    "2. **Normalize longitude column name** from `LONGITUD` ‚Üí `LONGITUDE` (FARS uses truncated SAS names).\n",
    "3. **Filter coordinates** to remove zeros, nulls, and non-CONUS locations.\n",
    "4. **Tag each record** with `location_type` (`'crash_exact'` or `'state_centroid'`) for transparency.\n",
    "5. **Save the final dataset** as `fars_daily_hybrid_coords.csv`.\n",
    "\n",
    "This hybrid approach ensures **maximal spatial accuracy** where possible while preserving **full historical coverage**, forming the foundation for high-resolution weather‚Äìcrash analysis at the daily level."
   ],
   "id": "f7082d5420c4cd15"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T10:26:06.947865Z",
     "start_time": "2025-11-19T10:25:56.810064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### STEP 2: Build hybrid daily crash dataset (state-level before 2000, crash-level after)\n",
    "def parse_hybrid_daily():\n",
    "    all_crashes = []\n",
    "\n",
    "    for year_folder in tqdm(sorted(data_folder.iterdir()), desc=\"Parsing FARS\"):\n",
    "        if not year_folder.is_dir():\n",
    "            continue\n",
    "\n",
    "        acc_file = year_folder / 'accident.csv'\n",
    "        if not acc_file.exists():\n",
    "            continue\n",
    "\n",
    "        year = int(year_folder.name)\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(acc_file, low_memory=False, encoding='utf-8')\n",
    "        except:\n",
    "            df = pd.read_csv(acc_file, low_memory=False, encoding='latin1')\n",
    "\n",
    "        # Parse date\n",
    "        if all(col in df.columns for col in ['YEAR', 'MONTH', 'DAY']):\n",
    "            df = df[(df['DAY'] > 0) & (df['MONTH'] > 0)]\n",
    "            df['date'] = pd.to_datetime(df[['YEAR', 'MONTH', 'DAY']], errors='coerce')\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if 'STATE' not in df.columns:\n",
    "            continue\n",
    "\n",
    "        if year >= 2000 and 'LATITUDE' in df.columns and 'LONGITUD' in df.columns:\n",
    "            # üìç Use actual crash coordinates (2000‚Äì2023)\n",
    "            df = df.rename(columns={'LONGITUD': 'LONGITUDE'})\n",
    "            df = df.dropna(subset=['LATITUDE', 'LONGITUDE'])\n",
    "            df = df[(df['LATITUDE'] != 0) & (df['LONGITUDE'] != 0)]\n",
    "            df = df[\n",
    "                (df['LATITUDE'].between(24, 50)) &\n",
    "                (df['LONGITUDE'].between(-125, -66))\n",
    "                ]\n",
    "            df['lat'] = df['LATITUDE']\n",
    "            df['lon'] = df['LONGITUDE']\n",
    "            df['location_type'] = 'crash_exact'\n",
    "\n",
    "        else:\n",
    "            # Use state centroid (1975‚Äì1999)\n",
    "            df['lat'] = df['STATE'].map({k: v[0] for k, v in STATE_CENTROIDS.items()})\n",
    "            df['lon'] = df['STATE'].map({k: v[1] for k, v in STATE_CENTROIDS.items()})\n",
    "            df['location_type'] = 'state_centroid'\n",
    "\n",
    "        # Keep one row per crash\n",
    "        crashes = df[['date', 'STATE', 'lat', 'lon', 'location_type']].copy()\n",
    "        crashes = crashes.dropna(subset=['date', 'lat', 'lon'])\n",
    "        all_crashes.append(crashes)\n",
    "\n",
    "    return pd.concat(all_crashes, ignore_index=True)\n",
    "\n",
    "\n",
    "# Run it\n",
    "df_crashes = parse_hybrid_daily()\n",
    "print(f\"Parsed {len(df_crashes):,} crashes\")\n",
    "print(f\"Location types: {df_crashes['location_type'].value_counts().to_dict()}\")\n",
    "\n",
    "# Save\n",
    "hybrid_file = data_folder / \"fars_daily_hybrid_coords.csv\"\n",
    "df_crashes.to_csv(hybrid_file, index=False)"
   ],
   "id": "61a23be4b3e0c93",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing FARS: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:08<00:00,  7.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 874,852 crashes\n",
      "Location types: {'crash_exact': 532088, 'state_centroid': 342764}\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T10:26:35.696179Z",
     "start_time": "2025-11-19T10:26:35.673307Z"
    }
   },
   "cell_type": "code",
   "source": "df_crashes.head()",
   "id": "c7745191a0df096",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        date  STATE      lat      lon   location_type\n",
       "0 1998-01-01      1  32.3182 -86.9023  state_centroid\n",
       "1 1998-01-01      1  32.3182 -86.9023  state_centroid\n",
       "2 1998-01-01      1  32.3182 -86.9023  state_centroid\n",
       "3 1998-01-03      1  32.3182 -86.9023  state_centroid\n",
       "4 1998-01-05      1  32.3182 -86.9023  state_centroid"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>STATE</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>location_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1998-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>32.3182</td>\n",
       "      <td>-86.9023</td>\n",
       "      <td>state_centroid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1998-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>32.3182</td>\n",
       "      <td>-86.9023</td>\n",
       "      <td>state_centroid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1998-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>32.3182</td>\n",
       "      <td>-86.9023</td>\n",
       "      <td>state_centroid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1998-01-03</td>\n",
       "      <td>1</td>\n",
       "      <td>32.3182</td>\n",
       "      <td>-86.9023</td>\n",
       "      <td>state_centroid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1998-01-05</td>\n",
       "      <td>1</td>\n",
       "      <td>32.3182</td>\n",
       "      <td>-86.9023</td>\n",
       "      <td>state_centroid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T10:58:37.872497Z",
     "start_time": "2025-11-19T10:58:37.496223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Load FARS data and filter for 6 target states\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "project_root = Path(os.getcwd()).parents[0]\n",
    "data_folder = project_root / 'data' / 'data_accidents'\n",
    "\n",
    "# Load full dataset (already with grid assignments)\n",
    "df_crashes = pd.read_csv(data_folder / \"fars_daily_hybrid_coords.csv\", parse_dates=['date'])\n",
    "\n",
    "# Define target states (FIPS codes)\n",
    "TARGET_STATES = [6, 48, 12, 36, 17, 53]  # CA, TX, FL, NY, IL, WA\n",
    "\n",
    "# Filter to only these states\n",
    "df_subset = df_crashes[df_crashes['STATE'].isin(TARGET_STATES)].copy()\n",
    "\n",
    "print(f\"Filtered to {len(df_subset):,} crashes from {len(TARGET_STATES)} states\")"
   ],
   "id": "738b4b024c405985",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered to 303,425 crashes from 6 states\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T10:58:56.377657Z",
     "start_time": "2025-11-19T10:58:55.279498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Assign crashes to 1.0¬∞ grid (111km x 88km cells)\n",
    "def assign_to_grid(df, grid_size=1.0):\n",
    "    df = df.copy()\n",
    "    df['grid_lat'] = (df['lat'] // grid_size) * grid_size + grid_size / 2\n",
    "    df['grid_lon'] = (df['lon'] // grid_size) * grid_size + grid_size / 2\n",
    "    df['grid_id'] = (\n",
    "            df['grid_lat'].round(3).astype(str) + '_' +\n",
    "            df['grid_lon'].round(3).astype(str)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "df_subset = assign_to_grid(df_subset, grid_size=1.0)\n",
    "\n",
    "# Save filtered dataset\n",
    "output_path = data_folder / \"fars_daily_6states.csv\"\n",
    "df_subset.to_csv(output_path, index=False)\n",
    "print(f\"Saved {len(df_subset):,} crashes to {output_path}\")\n",
    "print(f\"Total unique grid cells: {df_subset['grid_id'].nunique()}\")  # Expect: 60‚Äì100"
   ],
   "id": "1a54884d0763a4f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 303,425 crashes to /Users/patricijamarijanovic/AVEIRO/FCD/foundations-of-data-science/data/data_accidents/fars_daily_6states.csv\n",
      "Total unique grid cells: 251\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T11:02:46.951096Z",
     "start_time": "2025-11-19T11:02:11.934646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Fetch historical weather for just these 60‚Äì100 grid cells\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "\n",
    "# Setup Open-Meteo client\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after=86400)\n",
    "retry_session = retry(cache_session, retries=3, backoff_factor=0.5)\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "grid_weather_dir = data_folder / \"grid_weather_6states\"\n",
    "os.makedirs(grid_weather_dir, exist_ok=True)\n",
    "\n",
    "# Get unique grid cells\n",
    "unique_grids = df_subset[['grid_id', 'grid_lat', 'grid_lon']].drop_duplicates().reset_index(drop=True)\n",
    "print(f\"Fetching weather for {len(unique_grids)} grid cells...\")\n",
    "\n",
    "for _, row in tqdm(unique_grids.iterrows(), total=len(unique_grids), desc=\"Fetching weather\"):\n",
    "    grid_id = row['grid_id']\n",
    "    lat = row['grid_lat']\n",
    "    lon = row['grid_lon']\n",
    "    output_path = grid_weather_dir / f\"{grid_id}.parquet\"\n",
    "\n",
    "    # Skip if already downloaded\n",
    "    if output_path.exists():\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "        params = {\n",
    "            \"latitude\": lat,\n",
    "            \"longitude\": lon,\n",
    "            \"start_date\": \"1975-01-01\",\n",
    "            \"end_date\": \"2023-12-31\",\n",
    "            \"daily\": [\"temperature_2m_mean\", \"precipitation_sum\", \"wind_speed_10m_mean\"],\n",
    "            \"timezone\": \"UTC\"\n",
    "        }\n",
    "\n",
    "        responses = openmeteo.weather_api(url, params=params)\n",
    "        response = responses[0]\n",
    "\n",
    "        daily = response.Daily()\n",
    "        dates = pd.date_range(\n",
    "            start=pd.to_datetime(daily.Time(), unit=\"s\", utc=True),\n",
    "            end=pd.to_datetime(daily.TimeEnd(), unit=\"s\", utc=True),\n",
    "            freq=pd.Timedelta(seconds=daily.Interval()),\n",
    "            inclusive=\"left\"\n",
    "        )\n",
    "\n",
    "        df_weather = pd.DataFrame({\n",
    "            \"date\": dates.strftime('%Y-%m-%d'),\n",
    "            \"temperature_2m_mean\": daily.Variables(0).ValuesAsNumpy(),\n",
    "            \"precipitation_sum\": daily.Variables(1).ValuesAsNumpy(),\n",
    "            \"wind_speed_10m_mean\": daily.Variables(2).ValuesAsNumpy(),\n",
    "            \"grid_id\": grid_id\n",
    "        })\n",
    "\n",
    "        df_weather.to_parquet(output_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {grid_id}: {e}\")\n",
    "        with open(grid_weather_dir / \"failed_grids.txt\", \"a\") as f:\n",
    "            f.write(f\"{grid_id},{lat},{lon},{str(e)}\\n\")\n",
    "\n",
    "\n",
    "    time.sleep(8)\n",
    "\n",
    "print(\"Weather data downloaded !\")"
   ],
   "id": "9f45426411ec1e47",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching weather for 251 grid cells...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching weather:   2%|‚ñè         | 4/251 [00:24<26:53,  6.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching 31.5_-99.5: failed to request 'https://archive-api.open-meteo.com/v1/archive': {'reason': 'Minutely API request limit exceeded. Please try again in one minute.', 'error': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching weather:   2%|‚ñè         | 5/251 [00:32<28:47,  7.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching 47.5_-120.5: failed to request 'https://archive-api.open-meteo.com/v1/archive': {'error': True, 'reason': 'Minutely API request limit exceeded. Please try again in one minute.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching weather:   2%|‚ñè         | 5/251 [00:34<28:36,  6.98s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[25], line 68\u001B[0m\n\u001B[1;32m     65\u001B[0m             f\u001B[38;5;241m.\u001B[39mwrite(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgrid_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlat\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlon\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(e)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     67\u001B[0m     \u001B[38;5;66;03m# Very fast: 1 second delay = 60 requests/hour ‚Üí safe for <100 cells\u001B[39;00m\n\u001B[0;32m---> 68\u001B[0m     \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m8\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWeather data downloaded in under 5 minutes!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
